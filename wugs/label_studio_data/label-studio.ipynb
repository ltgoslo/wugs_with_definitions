{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946cd648-950e-4f4c-8c5c-e724ed768d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import csv\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ec44455-e347-4a76-8415-130ebede9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd9de46-757e-4f7f-ad57-cec1a206b95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_clusters(predictions_folder):\n",
    "    word_clusters_dict = {}\n",
    "    for word_path in glob(predictions_folder):\n",
    "        word = os.path.split(word_path)[-1]\n",
    "    \n",
    "        word_uses = pd.read_csv(os.path.join(DWUG_PATH, \"data\", word, \"uses.csv\"), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        \n",
    "        clusters_dir = os.path.join(DWUG_PATH, \"clusters/opt\")\n",
    "        if not os.path.exists(clusters_dir):  # no opt in RuDSI data\n",
    "            clusters_dir = os.path.join(DWUG_PATH, \"clusters\")\n",
    "        word_clusters = pd.read_csv(os.path.join(clusters_dir, f\"{word}.csv\"), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "        word_clusters[CLUSTER_NUMBER_COLUMN] = word_clusters[CLUSTER_NUMBER_COLUMN].astype(int)\n",
    "    \n",
    "        \n",
    "        this_word = word_uses.join(word_clusters.set_index(\"identifier\"), on=\"identifier\")\n",
    "        \n",
    "        word_clusters_dict[word] = this_word\n",
    "    return word_clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7dffe58-dab2-41d5-a1c1-5af44983a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_folder(predictions_folder, label_data, word_clusters_dict, word_definitions_dict):\n",
    "    clusters_minus_1 = 0\n",
    "    one_use_clusters = 0\n",
    "    two_use_cluster = 0\n",
    "    one_clusters = 0\n",
    "    one_clusters_after_one_use_removal = 0\n",
    "    all_definitions = set()\n",
    "    all_definitions_by_word = defaultdict(set)\n",
    "    for word_path in glob(predictions_folder):\n",
    "        word = os.path.split(word_path)[-1]\n",
    "        other_methods_word_definitions = set()\n",
    "        for definitions_set in word_definitions_dict[word]:\n",
    "            other_methods_word_definitions = other_methods_word_definitions.union(definitions_set)\n",
    "        \n",
    "        \n",
    "        clusters_and_definitions = pd.read_csv(os.path.join(word_path, \"cluster_gloss.tsv\"), sep=\"\\t\")\n",
    "        clusters_and_definitions[\"cluster\"] = clusters_and_definitions.cluster.astype(int)\n",
    "        clusters_minus_1 += clusters_and_definitions[clusters_and_definitions.cluster==-1].shape[0]\n",
    "        clusters_and_definitions = clusters_and_definitions[clusters_and_definitions.cluster!=-1]\n",
    "        \n",
    "        if clusters_and_definitions.shape[0] > 1:\n",
    "            this_word = word_clusters_dict[word]\n",
    "        \n",
    "            definitions, contexts_list, contexts_list_html = [], [], []\n",
    "            for row in clusters_and_definitions.iterrows():\n",
    "                cluster_number, definition = row[1]\n",
    "                if definition not in all_definitions:\n",
    "                        all_definitions.add(definition)\n",
    "                        all_definitions_by_word[word].add(definition)\n",
    "                else: \n",
    "                    words = [word]\n",
    "                    for word_key, word_definitions in all_definitions_by_word.items():\n",
    "                        if (word_key != word) and (definition in word_definitions):\n",
    "                            words.append(word_key)\n",
    "\n",
    "                    if len(words)>1:\n",
    "                        print(f\"Common definition {definition} for words {words} by {predictions_folder}\")\n",
    "                \n",
    "                    \n",
    "                        \n",
    "                this_cluster = this_word[this_word[CLUSTER_NUMBER_COLUMN] == cluster_number]\n",
    "                if this_cluster.shape[0] == 2:\n",
    "                    two_use_cluster += 1\n",
    "                if this_cluster.shape[0] == 1:\n",
    "                    one_use_clusters += 1\n",
    "                if this_cluster.shape[0] > 2:\n",
    "                    \n",
    "                    definitions.append(definition)\n",
    "                    contexts = []\n",
    "                    \n",
    "                    for row in this_cluster.iterrows():\n",
    "                        start, end = row[1][\"indexes_target_token\"].split(\":\")\n",
    "                        start, end = int(start), int(end)\n",
    "                        example = row[1][EXAMPLE_COLUMN]\n",
    "                        contexts.append(f\"- {example[:start]}<b>{example[start:end]}</b>{example[end:]}\")\n",
    "        \n",
    "                    if not contexts: \n",
    "                        print(word)\n",
    "                        print(cluster_number)\n",
    "                    try:\n",
    "                        choiced_contexts = random.sample(contexts, k=min(len(contexts), 5))\n",
    "                        \n",
    "                        contexts_list.append(str(cluster_number))\n",
    "                        contexts_list_html.append('<br>'.join(choiced_contexts))\n",
    "                        \n",
    "                    except IndexError:\n",
    "                        print(contexts)\n",
    "                        raise IndexError\n",
    "                \n",
    "\n",
    "            if (len(contexts_list) > 1):\n",
    "                seen_definitions = []\n",
    "                for definition in definitions:\n",
    "                    if (definition not in seen_definitions) and (definition not in other_methods_word_definitions):\n",
    "                        seen_definitions.append(definition)\n",
    "                        cluster_data = {}\n",
    "                        cluster_data[\"data\"] = {\"my_text\": f\"{word.upper()}: <b>{definition.upper()}</b>\"}\n",
    "                        \n",
    "                        \n",
    "                        cluster_data[\"data\"][\"variants\"] = [\n",
    "                            {\"value\": ctx, \"html\": ctx_html} for ctx, ctx_html in zip(contexts_list, contexts_list_html)\n",
    "                        ]\n",
    "                        random.shuffle(cluster_data[\"data\"][\"variants\"])\n",
    "                        cluster_data[\"data\"][\"variants\"].extend(\n",
    "                            [\n",
    "                                {\"value\": \"-2\", \"html\": '<b>This definition describes none of the clusters</b>'},\n",
    "                                {\"value\": \"-3\", \"html\": '<b>This definition describes more than one cluster</b>'},\n",
    "                            ],\n",
    "                        )\n",
    "                        label_data.append(cluster_data)\n",
    "                word_definitions_dict[word].append(set(seen_definitions))\n",
    "            else:\n",
    "                \n",
    "                one_clusters_after_one_use_removal += 1\n",
    "                \n",
    "                \n",
    "        else:\n",
    "            one_clusters += 1\n",
    "    print(f\"Number of clusters labeled with -1: {clusters_minus_1}\")\n",
    "    print(f\"Number of singleton clusters: {one_use_clusters}\")\n",
    "    print(f\"Number of clusters with two uses: {two_use_cluster}\")\n",
    "    print(f\"Number of words with one cluster only: {one_clusters}\")\n",
    "    print(f\"Number of words where one cluster only remained after removing singletons, -1: {one_clusters_after_one_use_removal}\")\n",
    "    return label_data, word_definitions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39cab585-0e23-4e9d-8352-698ba3466b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common definition the front of the human head from the forehead to the chin and ear to ear for words ['face_nn', 'head_nn'] by /home/m/PycharmProjects/gloss-annotator/predictions/pilot_glmlarge_wordnet_l1norm_top3/dwug_en/*\n",
      "Common definition a large scale offensive (more than a counterattack) undertaken by a defending force to seize the initiative from an attacking force for words ['attack_nn', 'head_nn'] by /home/m/PycharmProjects/gloss-annotator/predictions/pilot_glmlarge_wordnet_l1norm_top3/dwug_en/*\n",
      "Common definition the act of pressing; the exertion of pressure for words ['twist_nn', 'stab_nn'] by /home/m/PycharmProjects/gloss-annotator/predictions/pilot_glmlarge_wordnet_l1norm_top3/dwug_en/*\n",
      "Number of clusters labeled with -1: 33\n",
      "Number of singleton clusters: 326\n",
      "Number of clusters with two uses: 37\n",
      "Number of words with one cluster only: 4\n",
      "Number of words where one cluster only remained after removing singletons, -1: 12\n",
      "Number of clusters labeled with -1: 0\n",
      "Number of singleton clusters: 0\n",
      "Number of clusters with two uses: 0\n",
      "Number of words with one cluster only: 16\n",
      "Number of words where one cluster only remained after removing singletons, -1: 0\n",
      "Number of clusters labeled with -1: 33\n",
      "Number of singleton clusters: 326\n",
      "Number of clusters with two uses: 37\n",
      "Number of words with one cluster only: 4\n",
      "Number of words where one cluster only remained after removing singletons, -1: 12\n",
      "278 examples to annotate\n"
     ]
    }
   ],
   "source": [
    "label_data = []\n",
    "lang = \"en\"\n",
    "DWUG_PATH = os.path.expanduser(f\"~/PycharmProjects/gloss-annotator/wugs/dwug_{lang}/\")\n",
    "CLUSTER_NUMBER_COLUMN = 'cluster'\n",
    "EXAMPLE_COLUMN = 'context'\n",
    "WORD_COLUMN = 'word'\n",
    "predictions_folder = os.path.expanduser(\"~/PycharmProjects/gloss-annotator/predictions/\")\n",
    "methods = (\n",
    "    \"pilot_glmlarge_wordnet_l1norm_top3\",\n",
    "    \"pilot_flan-t5-definition-en-xl\",\n",
    "    \"lesk\",\n",
    ")\n",
    "word_clusters_dict = get_word_clusters(os.path.join(predictions_folder, f\"pilot_glmlarge_wordnet_l1norm_top3/dwug_{lang}/*\"))\n",
    "word_definitions_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "label_data, word_definitions_dict = pass_folder(\n",
    "    os.path.join(predictions_folder, f\"pilot_glmlarge_wordnet_l1norm_top3/dwug_{lang}/*\"),\n",
    "    label_data,\n",
    "    word_clusters_dict,\n",
    "    word_definitions_dict,\n",
    ")\n",
    "label_data, word_definitions_dict = pass_folder(\n",
    "    os.path.join(\n",
    "    predictions_folder,\n",
    "    f\"pilot_flan-t5-definition-en-xl/dwug_{lang}/*\",\n",
    "    ),\n",
    "    label_data,\n",
    "    word_clusters_dict,\n",
    "    word_definitions_dict,\n",
    ")\n",
    "label_data, word_definitions_dict = pass_folder(\n",
    "    os.path.join(\n",
    "    predictions_folder,\n",
    "    f\"lesk/dwug_{lang}/*\",\n",
    "    ),\n",
    "    label_data,\n",
    "    word_clusters_dict,\n",
    "    word_definitions_dict,\n",
    ")\n",
    "print(f\"{len(label_data)} examples to annotate\")\n",
    "\n",
    "random.shuffle(label_data)\n",
    "with open(os.path.expanduser(f\"~/PycharmProjects/label-studio-{lang}.json\"), \"w\") as f:\n",
    "    json.dump(label_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34ef1404-bcdc-49f1-84e2-ef5338f235b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contemplation_nn\n",
      "ounce_nn\n",
      "fiction_nn\n",
      "bag_nn\n",
      "afternoon_nn\n",
      "quilt_nn\n",
      "5\n",
      "5\n",
      "player_nn\n",
      "12\n",
      "12\n",
      "prop_nn\n",
      "12\n",
      "12\n",
      "ball_nn\n",
      "12\n",
      "12\n",
      "head_nn\n",
      "10\n",
      "10\n",
      "lass_nn\n",
      "5\n",
      "5\n",
      "stroke_vb\n",
      "7\n",
      "7\n",
      "relationship_nn\n",
      "5\n",
      "5\n",
      "grain_nn\n",
      "11\n",
      "11\n",
      "lane_nn\n",
      "stab_nn\n",
      "9\n",
      "9\n",
      "tree_nn\n",
      "rally_nn\n",
      "5\n",
      "5\n",
      "graft_nn\n",
      "5\n",
      "5\n",
      "face_nn\n",
      "5\n",
      "5\n",
      "record_nn\n",
      "7\n",
      "7\n",
      "bit_nn\n",
      "11\n",
      "11\n",
      "land_nn\n",
      "7\n",
      "7\n",
      "risk_nn\n",
      "heel_nn\n",
      "7\n",
      "7\n",
      "chairman_nn\n",
      "edge_nn\n",
      "16\n",
      "16\n",
      "circle_vb\n",
      "thump_nn\n",
      "plane_nn\n",
      "14\n",
      "14\n",
      "attack_nn\n",
      "6\n",
      "6\n",
      "tip_vb\n",
      "15\n",
      "15\n",
      "word_nn\n",
      "9\n",
      "9\n",
      "bar_nn\n",
      "17\n",
      "17\n",
      "savage_nn\n",
      "pin_vb\n",
      "9\n",
      "9\n",
      "rag_nn\n",
      "8\n",
      "8\n",
      "pick_vb\n",
      "15\n",
      "15\n",
      "donkey_nn\n",
      "gas_nn\n",
      "6\n",
      "6\n",
      "include_vb\n",
      "maxim_nn\n",
      "multitude_nn\n",
      "part_nn\n",
      "9\n",
      "9\n",
      "chef_nn\n",
      "8\n",
      "8\n",
      "twist_nn\n",
      "11\n",
      "11\n",
      "278\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "before, after = 0, 0\n",
    "for word, definitions in word_definitions_dict.items():\n",
    "    print(word)\n",
    "    if definitions:\n",
    "        current_unique = len(definitions[0]) + len(definitions[1]) + len(definitions[2])\n",
    "        before += current_unique\n",
    "        print(current_unique)\n",
    "        new_unique = len((definitions[0].union(definitions[1])).union(definitions[2]))\n",
    "        after += new_unique\n",
    "        print(new_unique)\n",
    "        if current_unique != new_unique:\n",
    "            for method, method_definitions in zip(methods, definitions):\n",
    "                print(method)\n",
    "                print(method_definitions)\n",
    "print(before)\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94f07d6-882d-4bd2-869e-9e54a3ff911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser(f\"~/PycharmProjects/label-studio-{lang}-test.json\"), \"w\") as f:\n",
    "    json.dump(label_data[:15], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297890e0-9afc-4b48-a937-8a400560fcda",
   "metadata": {},
   "source": [
    "Labeling interface - https://github.com/ltgoslo/gloss-annotator/blob/main/wugs/label_studio_data/labeling_config.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glossannotator",
   "language": "python",
   "name": "glossannotator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
